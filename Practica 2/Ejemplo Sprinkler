Esto Esta copiado de la consola por tanto para usar los comandos hay que quitar >> y si no tienen >> son salidas por pantalla y si tienen // son comentarios mios de la instruccion o salida por pantalla


addpath('~/asigDSIC/ETSINF/apr/p2/BNT')
addpath(genpathKPM('~/asigDSIC/ETSINF/apr/p2/BNT'))
N = 4; C = 1; S = 2; R = 3; W = 4;
grafo = zeros(N, N);
grafo(C,[R S]) = 1;
grafo(R,W)     = 1;
grafo(S,W)     = 1;
grafo(grafo(C,[R S]))
nodosDiscretos = 1:N;
tallaNodos = 2*ones(1,N);
redB = mk_bnet(grafo, tallaNodos, 'discrete', nodosDiscretos);
redB.CPD{C} = tabular_CPD(redB, C, [0.5 0.5]);
redB.CPD{S} = tabular_CPD(redB, S, [0.5 0.9 0.5 0.1]);
redB.CPD{R} = tabular_CPD(redB, R, [0.8 0.2 0.2 0.8]);
redB.CPD{W} = tabular_CPD(redB, W, [1.0 0.1 0.1 0.01 0.0 0.9 0.9 0.99]);
//inferencia
motor = jtree_inf_engine(redB);
//Distribuciones condicionales                      
evidencia = cell(1, N);                                                 
evidencia{W} = 2;                                                       
[motor, logVerosim] = enter_evidence(motor, evidencia);
m = marginal_nodes(motor, S);
m

m = 

  struct with fields:

    domain: 2
         T: [2Ã—1 double]
        mu: []
     Sigma: []

m.T

ans =

    0.5702 //=> P(S = 1|W= 2)
    0.4298 //=> P(S = 2|W= 2)

evidencia{R} = 2;
evidencia{W} = 2;
motor, logVerosim] = enter_evidence(motor, evidencia);
m = marginal_nodes(motor, S);                          
m.T                                                    

ans =

    0.8055 //=> P(S = 1|R= 2,W= 2)
    0.1945 //=> P(S = 2|R= 2,W= 2)
>> 
// Nodos observados
evidencia = cell(1, N);
evidencia{W} = 2;
motor = enter_evidence(motor, evidencia);
m = marginal_nodes(motor, W);
m.T

ans =

    1.0000

m = marginal_nodes(motor, W, 1);
m.T                             

ans =

         0 //=> P(W = 1|W= 2)
    1.0000 //=> P(W = 2|W= 2)
// Distribuciones Conjuntas

evidencia = cell(1,N);
[motor, logVerosimi] = enter_evidence(motor, evidencia);
m = marginal_nodes(motor, [S R W]);
m.T
//ans(2,1,1) = P(S=2,R=1,W=1=
ans(:,:,1) =

    0.2900 //(1,1,1)    0.0410//(1,2,1)
    0.0210    0.0009


ans(:,:,2) =

    0	//(1,1,2)    0.3690 // (1,2,2)
    0.1890    		 0.0891

// Aprendizaje

semilla = 0; rng(semilla);
nMuestras = 1000;
muestras = cell(N, nMuestras);
for i=1:nMuestras muestras(:,i) = sample_bnet(redB); end

//La otra red

redAPR = mk_bnet(grafo, tallaNodos);
redAPR.CPD{C} = tabular_CPD(redAPR, C);
redAPR.CPD{R} = tabular_CPD(redAPR, R);
redAPR.CPD{S} = tabular_CPD(redAPR, S);
redAPR.CPD{W} = tabular_CPD(redAPR, W);
redAPR2=learn_params(redAPR, muestras);
TPCaux = cell(1,N);
for i=1:N s=struct(redAPR2.CPD{i}); TPCaux{i}=s.CPT; end
dispcpt(TPCaux{W}) // Probabilidad estimada del nodo W

// Aprendizaje con datos incompletos mediante EM

muestrasS = muestras;
semilla = 0; rng(semilla);
ocultas = rand(N, nMuestras) > 0.5;
[I,J] = find(ocultas);
for k=1:length(I) muestrasS{I(k), J(k)} = []; end

//Preparamos una nueva red cuyas probabilidades van a ser re-estimadas por EM:

maxIter = 1000; eps = 1e-4;
semilla = 0; rng(semilla);
[redEM2, trazaLogVer] = learn_params_em(motorEM, muestrasS, maxIter, eps);
auxTPC = cell(1,N);
for i=1:N s=struct(redEM2.CPD{i}); auxTPC{i}=s.CPT; end

dispcpt(auxTPC{S})

dispcpt(auxTPC{W})


